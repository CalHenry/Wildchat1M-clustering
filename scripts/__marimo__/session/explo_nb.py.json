{
  "version": "1",
  "metadata": {
    "marimo_version": "0.17.7"
  },
  "cells": [
    {
      "id": "vblA",
      "code_hash": "2ea4e1ba1009d5f0b55220b35c236049",
      "outputs": [
        {
          "type": "data",
          "data": {
            "text/markdown": "<span class=\"markdown prose dark:prose-invert contents\"><h2 id=\"data-preparation\">Data preparation</h2>\n<h3 id=\"a-word-on-the-libraries-used\">A word on the libraries used:</h3>\n<span class=\"paragraph\"><strong>Duckdb</strong> is great and <strong>Polars</strong> is awsome.</span>\n<span class=\"paragraph\">I could use Duckdb only and write SQL queries but I am more confortable with <strong>Polars's</strong> synthax for complex and chainned modifications to dataframes.\nThey both have lazy excecution and are very fast.</span>\n<span class=\"paragraph\">I can use:</span>\n<ul>\n<li>SQL at the begining, to import data or a filtered version from raw files. Simple queries can do a lot.</li>\n<li>Polars for the complex manipulations in the workflow.</li>\n<li>SQL and Duckdb for big aggregations and joins.</li>\n</ul>\n<span class=\"paragraph\">Benefits of using duckdb in my workflow:</span>\n<ul>\n<li>reading data is extremly fast (although polars is very fast as well)</li>\n<li>can import in a  super efficient way with just a simple SQL query</li>\n<li>SQL relations are similar to polars's expressions and can be very handy</li>\n<li>SQL views are also very handy</li>\n</ul>\n<span class=\"paragraph\">\u27a1\ufe0f Both libraries are interchangeable here and I can choose the one I prefer for each part of my work and keep a seamless workflow.</span></span>"
          }
        }
      ],
      "console": []
    },
    {
      "id": "bkHC",
      "code_hash": "d5f24ee654dfdc47fdd4e197e5d22f70",
      "outputs": [
        {
          "type": "data",
          "data": {
            "text/markdown": "<span class=\"markdown prose dark:prose-invert contents\"><hr /></span>"
          }
        }
      ],
      "console": []
    },
    {
      "id": "lEQa",
      "code_hash": "0db88638c5343ea2e7b83474b8be84dc",
      "outputs": [
        {
          "type": "data",
          "data": {
            "text/markdown": "<span class=\"markdown prose dark:prose-invert contents\"><span class=\"paragraph\">Next cell is a great example of seemless integration between the 2 libraries:</span>\n<ol>\n<li>We use duckdb for a SQL query</li>\n<li>We specify that we want to convert the query's result to Polars df.\nAs soon as <code>.pl(lazy=True)</code> is written, the object is now a polars LazyFrame, to which i can apply polars code.\nI used both libraries in the same sequence.</li>\n</ol>\n<blockquote>\n<span class=\"paragraph\">I'll use the abbreviation '<strong>lf</strong>' for lazyframes to differentiate with dataframes</span>\n</blockquote></span>"
          }
        }
      ],
      "console": []
    },
    {
      "id": "Xref",
      "code_hash": "749128a3f217e430063a29b6972547d0",
      "outputs": [
        {
          "type": "data",
          "data": {
            "text/markdown": "<span class=\"markdown prose dark:prose-invert contents\"><h3 id=\"few-observations-on-the-raw-dataset\">Few observations on the raw dataset:</h3>\n<ul>\n<li>This dataset has 3 levels:</li>\n</ul>\n<ol>\n<li><strong>User</strong> level: identified by <strong>'hashed_ip'</strong> (based on the user's IP address)</li>\n<li><strong>Conversations</strong> level: identified by <strong>'conversation_hash'</strong>. <em>Uniquely</em> identified by <strong>'conversation_hash'</strong> + <strong>'hashed_ip'</strong>. ('conversation_hash' is not a unique identifier. 2 identical conversations will have the hash regarless the user, messages inside the conv will always have unique id)</li>\n<li><strong>Message</strong> level: identified <strong>'turn_identifier'</strong></li>\n</ol>\n<span class=\"paragraph\">1 user can have N conversations of N messages.\n1 message belong to 1 conversation that belong to 1 user</span>\n<ul>\n<li>Messages works in pairs: one <strong>user</strong> message, one <strong>assistant</strong> response. <strong>turn_identifier</strong> + <strong>role</strong> give the order of the messages. The <strong>user</strong> always comes before the <strong>assistant</strong>.</li>\n<li><strong>'conversations'</strong> is a complex variable of <strong>List[structs]</strong>. Each element of the list is 1 message. The fields of the <strong>struct</strong> are the message's content and it's metadata. Some fields are specific to the user, others to the assistant (expected missing values).</li>\n</ul>\n<h3 id=\"tidy-manipulation-variable-selection\">Tidy: manipulation, variable selection</h3>\n<span class=\"paragraph\">We want to be at the <strong>conversation level</strong>, but we don't want a complex schema of <strong>List[structs]</strong> inside a single variable like we have in the raw dataset (the list can be +100 elements for long conversations).\nLogic:</span>\n<ul>\n<li>flatten the dataset to message level, each row is a unique message, n rows for the same conversation.</li>\n<li>aggregate back to conversation level by creating features around the messages. We keep metadata variables for the conversations and not the messages.</li>\n<li>Final tidy dataset at the conversation level with several variables for different aggregations of the  messages's content (see section for details).</li>\n</ul>\n<h3 id=\"order-of-the-tasks\">Order of the tasks:</h3>\n<ol>\n<li>Import data, shortlist of variables, filter only English conversations</li>\n<li>User level --&gt; Message level (flatten the dataset)</li>\n<li>Unique ID for conversations</li>\n<li>Message level --&gt; Conversation level</li>\n</ol></span>"
          }
        }
      ],
      "console": []
    },
    {
      "id": "emfo",
      "code_hash": "d5f24ee654dfdc47fdd4e197e5d22f70",
      "outputs": [
        {
          "type": "data",
          "data": {
            "text/markdown": "<span class=\"markdown prose dark:prose-invert contents\"><hr /></span>"
          }
        }
      ],
      "console": []
    },
    {
      "id": "iLit",
      "code_hash": "e38d52c066f2c732407523cb975ecc82",
      "outputs": [
        {
          "type": "data",
          "data": {
            "text/markdown": "<span class=\"markdown prose dark:prose-invert contents\"><h2 id=\"data-cleaning\">Data cleaning</h2>\n<span class=\"paragraph\">Now that we have prepare the dataset by moving variables around, we can dig into the content and prepare the messages.</span>\n<span class=\"paragraph\">The idea is to remove any content that would worsen the clustering (REFAIREPHRASE) and increase the computation time.\nWe want to clean the data to optimize the computationnal steps (Tokenizations, Lemmatization)</span>\n<span class=\"paragraph\">Clean messages: (here the rows are of type <strong>str</strong>)</span>\n<ul>\n<li>remove emails, url and punctuation</li>\n<li>remove stopwords from the NLTK list of stopwords</li>\n<li>tokenize</li>\n</ul>\n<span class=\"paragraph\">Clean tokens: (here the rows are of type <strong>list[str]</strong>)</span>\n<ul>\n<li>remove empty tokens</li>\n<li>normalize numbers</li>\n<li>remove single characters tokens except a shortlist of word (like pronouns '<strong>I</strong>')</li>\n<li>maybe special characters if some found</li>\n<li>keep programming tokens ? (x, y , i)</li>\n<li>lemmatization (done last because takes longer, so done on fewer tokens as possible)</li>\n</ul></span>"
          }
        }
      ],
      "console": []
    },
    {
      "id": "ZHCJ",
      "code_hash": "ea924c5f8387510fb3dcc396f73f7953",
      "outputs": [
        {
          "type": "data",
          "data": {
            "text/markdown": "<span class=\"markdown prose dark:prose-invert contents\"><h3 id=\"messages-cleaning\">Messages cleaning</h3></span>"
          }
        }
      ],
      "console": []
    },
    {
      "id": "TqIu",
      "code_hash": "30a42a18812fc540eadbf9bcde026a83",
      "outputs": [
        {
          "type": "data",
          "data": {
            "text/markdown": "<span class=\"markdown prose dark:prose-invert contents\"><h3 id=\"conversations-cleaning\">Conversations cleaning</h3></span>"
          }
        }
      ],
      "console": []
    },
    {
      "id": "ulZA",
      "code_hash": "0f01c303fc90b9a390b75e8e30618f27",
      "outputs": [
        {
          "type": "data",
          "data": {
            "text/markdown": "<span class=\"markdown prose dark:prose-invert contents\"><span class=\"paragraph\"><strong>[^\\x00-\\x7F\\x80-\\xFF\\u0100-\\u017F\\u0180-\\u024F\\u1E00-\\u1EFF]</strong></span>\n<span class=\"paragraph\">This pattern removes:</span>\n<ul>\n<li>Greek letters (\u03b1, \u03b2, \u03b3)</li>\n<li>Cyrillic (\u0430, \u0431, \u0432)</li>\n<li>Chinese/Japanese/Korean characters</li>\n<li>Arabic, Hebrew, Thai, etc.</li>\n</ul>\n<span class=\"paragraph\">While keeping:</span>\n<ul>\n<li>All basic Latin letters and extended Latin with diacritics</li>\n<li>Spaces, punctuation, numbers (they're in the ASCII range)</li>\n</ul>\n<ol>\n<li><strong>\\x00-\\x7F</strong> (0-127): Basic ASCII\nIncludes A-Z, a-z, digits, punctuation, spaces, control characters</li>\n<li><strong>\\x80-\\xFF</strong> (128-255): Latin-1 Supplement\nIncludes: \u00e9, \u00f1, \u00fc, \u00e7, \u00e0, \u00f4, etc.\nCommon accented characters in Western European languages</li>\n<li><strong>\\u0100-\\u017F</strong>: Latin Extended-A\nIncludes: \u0101, \u0113, \u012b, \u0151, \u0171, \u0142, \u0144, \u015b, etc.\nUsed in Central/Eastern European languages (Polish, Czech, etc.)</li>\n<li><strong>\\u0180-\\u024F</strong>: Latin Extended-B\nIncludes: \u0192, \u01ce, \u01d0, \u01d2, \u01d4, \u0218, \u021a, etc.\nLess common Latin characters, some African languages</li>\n<li><strong>\\u1E00-\\u1EFF</strong>: Latin Extended Additional\nIncludes: \u1e40, \u1e41, \u1e80, \u1e81, \u1ef2, \u1ef3, Vietnamese characters, etc.\nSpecialized diacritics and Vietnamese</li>\n</ol></span>"
          }
        }
      ],
      "console": []
    },
    {
      "id": "aLJB",
      "code_hash": "a2b5bf0f6be8490f139dff1e76264f8d",
      "outputs": [
        {
          "type": "data",
          "data": {
            "text/markdown": "<span class=\"markdown prose dark:prose-invert contents\"><h2 id=\"data-visualization\">Data visualization</h2>\n<span class=\"paragraph\">Conversations are a set of messages between the user and the assistant (AI).\nWhat are baasic elements about the conversations and messages ?</span>\n<ul>\n<li>Number of turns (1 turn is a set of 1 message from the user and 1 message (the response) from the assistant)</li>\n<li>Number of message per conversations</li>\n<li>user's messages to assistant's messages ratio</li>\n<li>Lenght of messages/ conversation in tokens</li>\n</ul></span>"
          }
        }
      ],
      "console": []
    },
    {
      "id": "NCOB",
      "code_hash": "359c4076f1aca761f6e08ce735b1db52",
      "outputs": [
        {
          "type": "data",
          "data": {
            "text/markdown": "<span class=\"markdown prose dark:prose-invert contents\"><span class=\"paragraph\">We can see different kind of words:</span>\n<ul>\n<li>action verbs that tell the assistant what to do: write, use, make, give, create</li>\n<li>topic related words: file, code, script</li>\n</ul></span>"
          }
        }
      ],
      "console": []
    },
    {
      "id": "aqbW",
      "code_hash": "d5f24ee654dfdc47fdd4e197e5d22f70",
      "outputs": [
        {
          "type": "data",
          "data": {
            "text/markdown": "<span class=\"markdown prose dark:prose-invert contents\"><hr /></span>"
          }
        }
      ],
      "console": []
    },
    {
      "id": "TXez",
      "code_hash": "fdf0e129638a19241e564c3bb30d475f",
      "outputs": [
        {
          "type": "data",
          "data": {
            "text/markdown": "<span class=\"markdown prose dark:prose-invert contents\"><h1 id=\"test-zone\">Test zone</h1></span>"
          }
        }
      ],
      "console": []
    },
    {
      "id": "yCnT",
      "code_hash": "d5f24ee654dfdc47fdd4e197e5d22f70",
      "outputs": [
        {
          "type": "data",
          "data": {
            "text/markdown": "<span class=\"markdown prose dark:prose-invert contents\"><hr /></span>"
          }
        }
      ],
      "console": []
    },
    {
      "id": "kqZH",
      "code_hash": "d5f24ee654dfdc47fdd4e197e5d22f70",
      "outputs": [
        {
          "type": "data",
          "data": {
            "text/markdown": "<span class=\"markdown prose dark:prose-invert contents\"><hr /></span>"
          }
        }
      ],
      "console": []
    },
    {
      "id": "rEll",
      "code_hash": "d5f24ee654dfdc47fdd4e197e5d22f70",
      "outputs": [
        {
          "type": "data",
          "data": {
            "text/markdown": "<span class=\"markdown prose dark:prose-invert contents\"><hr /></span>"
          }
        }
      ],
      "console": []
    },
    {
      "id": "Hbol",
      "code_hash": "23f688a444c85a2ea56f1980a65be319",
      "outputs": [],
      "console": []
    },
    {
      "id": "MJUe",
      "code_hash": "d4f483eb964bda18011cdd754a39d651",
      "outputs": [],
      "console": []
    },
    {
      "id": "PKri",
      "code_hash": "c9ce25f2bde62ed3ff7a720e92963d60",
      "outputs": [],
      "console": []
    },
    {
      "id": "SFPL",
      "code_hash": "bb62148637dbecb563ef36762fed761c",
      "outputs": [],
      "console": []
    },
    {
      "id": "BYtC",
      "code_hash": "9f2b48d1c65fae4d4b1cc21db492459e",
      "outputs": [],
      "console": []
    },
    {
      "id": "RGSE",
      "code_hash": "d19d0f085aab6c11d936ac6da3f10061",
      "outputs": [],
      "console": []
    },
    {
      "id": "Kclp",
      "code_hash": "ed1da45ec40721178aa316d4093564a1",
      "outputs": [],
      "console": []
    },
    {
      "id": "Hstk",
      "code_hash": "d9bd0b339bb11a68c7fc6407059b4271",
      "outputs": [],
      "console": []
    },
    {
      "id": "nWHF",
      "code_hash": "63d5bb5d91d0ec1cc2dd9a44756b1796",
      "outputs": [],
      "console": []
    },
    {
      "id": "ROlb",
      "code_hash": "121ba5a3f59b382733a8ddbee12c64b2",
      "outputs": [],
      "console": []
    },
    {
      "id": "qnkX",
      "code_hash": "4091944dc10e4e6be81b0c4e69b39610",
      "outputs": [],
      "console": []
    },
    {
      "id": "Vxnm",
      "code_hash": "90606fa9dad533414f06af57e8b50f8d",
      "outputs": [],
      "console": []
    },
    {
      "id": "DnEU",
      "code_hash": "fde3911cb36b47a1d72a8ffa3d632a7f",
      "outputs": [],
      "console": []
    },
    {
      "id": "ecfG",
      "code_hash": "20949345d75d1095b288c790411509d2",
      "outputs": [],
      "console": []
    },
    {
      "id": "Pvdt",
      "code_hash": "2d8f36a8acce08ac61b118e3767e103b",
      "outputs": [],
      "console": []
    },
    {
      "id": "ZBYS",
      "code_hash": "5594cbb027e9af925aa96533034d6baf",
      "outputs": [],
      "console": []
    },
    {
      "id": "nHfw",
      "code_hash": "dfdc3cdbc5fd64fe627ed5f3692465ab",
      "outputs": [],
      "console": []
    },
    {
      "id": "xXTn",
      "code_hash": "2c8d1d2b4dae96220941e318cdb7b6b3",
      "outputs": [],
      "console": []
    },
    {
      "id": "AjVT",
      "code_hash": "09433307f4a966c606d3544d002a7607",
      "outputs": [],
      "console": []
    },
    {
      "id": "pHFh",
      "code_hash": "0515449c6eba8c73ea35759b8d7f95d6",
      "outputs": [],
      "console": []
    },
    {
      "id": "TRpd",
      "code_hash": "84748f5c61ca3d7fb90a613d7f6f64af",
      "outputs": [],
      "console": []
    },
    {
      "id": "dNNg",
      "code_hash": "b1d79c9cd1fca9aac115698cf76e5dc4",
      "outputs": [],
      "console": []
    },
    {
      "id": "wlCL",
      "code_hash": "048a849a56d4830af607c271efe77139",
      "outputs": [],
      "console": []
    },
    {
      "id": "wAgl",
      "code_hash": "6bd412628a556bf68e40279cbfbbc334",
      "outputs": [],
      "console": []
    },
    {
      "id": "dGlV",
      "code_hash": "ca18e0e7273b0e5025e9c34de8a9680a",
      "outputs": [],
      "console": []
    }
  ]
}